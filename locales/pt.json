{
  "chat": {
    "messages": {
      "thought_for_seconds": "Pensando por {second} segundo | \nPensando por {second} segundos",
      "thinking": "Pensando...",
      "reading": "Lendo",
      "search_locally": "Pesquisando localmente no navegador..."
    },
    "quick_actions": {
      "title": "A√ß√µes r√°pidas"
    },
    "input": {
      "placeholder": {
        "ask_anything": "Pergunte qualquer coisa...",
        "ask_follow_up": "Fa√ßa uma pergunta complementar..."
      },
      "tab_selector": {
        "all_tabs": "Todas as abas abertas"
      }
    },
    "prompt": {
      "highlight_key_insights": {
        "title": "Destacar insights principais"
      },
      "search_more": {
        "title": "Procure por mais conte√∫do como este"
      },
      "summarize_page_content": {
        "title": "Resumir a p√°gina"
      }
    }
  },
  "onboarding": {
    "banner": {
      "description": "Privado e seguro, com recursos poderosos que executam inteiramente em seu dispositivo.",
      "title": "AI no dispositivo, constru√≠do apenas para voc√™"
    },
    "guide": {
      "already_installed": "J√° instalou e est√° executando o Ollama?",
      "features": {
        "1": "Execute modelos avan√ßados como Deepseek, Qwen, Llama",
        "2": "Personalize e alterne entre modelos com controle total",
        "3": "Seus dados permanecem privados no seu dispositivo"
      },
      "follow_our_tutorial": "Siga nosso guia de instala√ß√£o",
      "get_ollama": "Obter Ollama",
      "install_desc": "Instale o Ollama para configurar sua IA local.",
      "need_help": "Precisa de ajuda?",
      "ollama_desc": "Desbloqueie todo o poder da IA local com Ollama.",
      "setup": "Configurar",
      "step1": "Etapa 1"
    },
    "webllm_tutorial": {
      "desc": "Use um modelo leve (QWEN 0.6B) diretamente no seu navegador - sem configura√ß√£o, sem espera.",
      "not_support_webllm": "WebLLM n√£o √© suportado no seu dispositivo",
      "start_with_webllm": "Comece com o modelo web",
      "title": "N√£o est√° pronto para instalar? \nExperimente instantaneamente."
    },
    "welcome_msg": {
      "title": "üëã Bem-vindo ao **NativeMind**",
      "body": "O NativeMind √© uma extens√£o de navegador com foco em privacidade que ajuda voc√™ a conversar, pesquisar e traduzir - tudo alimentado por modelos de linguagem no dispositivo.\n\n\nAqui est√° o que voc√™ pode fazer com o NativeMind:\n\n\n\n- Converse atrav√©s de v√°rias abas para acompanhar diferentes p√°ginas.\n\n- Pesquise na web diretamente no chat para obter mais contexto.\n\n- Clique com o bot√£o direito para traduzir instantaneamente qualquer parte da p√°gina.\n\n- Alterne ou baixe modelos a qualquer momento nas configura√ß√µes.\n\n\nVoc√™ pode come√ßar experimentando as a√ß√µes r√°pidas abaixo."
    }
  },
  "settings": {
    "advanced": {
      "title": "Avan√ßado"
    },
    "choose_model": "Escolha o modelo",
    "download": "Download",
    "feedback": {
      "contact_msg": "üìß D√∫vidas ou sugest√µes? Participe do nosso {discord} ou envie um email para {email}.",
      "join_waitlist": "üíº Quer us√°-lo no trabalho? {join_waitlist_link} para atualiza√ß√µes corporativas.",
      "join_waitlist_link": "Junte-se √† lista de espera",
      "discord": "Discord"
    },
    "get_ollama": "Obter Ollama",
    "models": {
      "title": "Modelo ativo"
    },
    "ollama": {
      "already_installed": "J√° instalou e est√° executando o Ollama?",
      "connected": "Conectado",
      "connection_error": "Erro de conex√£o",
      "downloadable_model": "Modelos dispon√≠veis para download",
      "follow_guide": "Siga nosso guia de instala√ß√£o",
      "learn_more_about_models": "Saiba mais sobre modelos",
      "need_help": "Precisa de ajuda?",
      "re_scan": "Escanear novamente",
      "server_address": "Endere√ßo do servidor",
      "setup": "Configurar",
      "unconnected": "Desconectado"
    },
    "prompts": {
      "chat_system_prompt": "Prompt de sistema do chat",
      "title": "Prompts",
      "translation_system_prompt": "Prompt de sistema do tradutor"
    },
    "provider": {
      "title": "Provedor de modelos"
    },
    "provider_model": {
      "title": "Provedor"
    },
    "quick_actions": {
      "description": "Configure a√ß√µes r√°pidas para executar seus prompts favoritos mais rapidamente - em um novo chat ou no menu de contexto.",
      "edit": {
        "cancel": "Cancelar",
        "prompt": "Prompt",
        "reset": "Redefinir",
        "save": "Salvar",
        "show_in_context_menu": "Mostrar no menu de contexto",
        "title": "T√≠tulo"
      },
      "title": "Personalizar a√ß√µes r√°pidas"
    },
    "test": "Teste",
    "translation": {
      "title": "Linguagem de tradu√ß√£o"
    },
    "webllm-desc": "Usando WebLLM para teste r√°pido. \nPara suporte completo aos modelos e melhor desempenho, instale o Ollama.",
    "general": {
      "system_language": "Idioma do sistema",
      "title": "Geral"
    },
    "title": "Configura√ß√µes"
  },
  "context_menu": {
    "quick_actions": {
      "title": "A√ß√µes r√°pidas"
    },
    "translation": {
      "show_original": "Mostrar original",
      "translate_page_into": "Traduzir esta p√°gina para {idioma}"
    }
  },
  "errors": {
    "model_not_found": "Ops! Algo deu errado. Por favor, verifique sua conex√£o com o Ollama nas configura√ß√µes e tente novamente.",
    "model_request_error": "Ops! Algo deu errado. Por favor, verifique sua conex√£o com o Ollama nas configura√ß√µes e tente novamente.",
    "model_request_timeout": "Tempo limite da solicita√ß√£o esgotado. Verifique sua conex√£o com o Ollama ou considere iniciar uma nova sess√£o, pois contextos longos podem afetar os tempos de resposta.",
    "timeout_error": "Opera√ß√£o expirou: {message}",
    "unknown_error": "Ocorreu um erro inesperado: {message}"
  }
}
